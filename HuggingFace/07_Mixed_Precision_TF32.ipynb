{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2617478344.py, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 45\u001b[0;36m\u001b[0m\n\u001b[0;31m    optimizer = AdamW(model=model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, 8_bit=True)\u001b[0m\n\u001b[0m                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# from torch_optimizer import AdamW, Adafactor\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 10\n",
    "lr = 5e-5\n",
    "batch_size = 8\n",
    "save_loc = '/home/arjun/Documents/ModelSaves/GPT2Alpaca'\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Dataset preparation and tokenizing\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "dataset = dataset['train']\n",
    "\n",
    "# Making dataset smaller for fast training\n",
    "dataset = dataset.select(range(100))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_loc)\n",
    "model = GPT2LMHeadModel.from_pretrained(save_loc)\n",
    "\n",
    "new_dataset = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "for example in dataset:\n",
    "    input_text = example['text'].replace('###', '')\n",
    "    encoded_data = tokenizer('' + input_text + '', truncation=True, max_length=768, padding=\"max_length\")\n",
    "    new_dataset['input_ids'].append(encoded_data['input_ids'])\n",
    "    new_dataset['attention_mask'].append(encoded_data['attention_mask'])\n",
    "\n",
    "new_dataset = Dataset.from_dict(new_dataset)\n",
    "new_dataset.set_format(\"torch\")\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(new_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model=model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, 8_bit=True)\n",
    "num_training_steps = num_epochs * len(dataloader)\n",
    "lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "# Mixed precision training setup\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "progress_bar = tqdm(range(num_training_steps-1), desc='Training', unit='steps')\n",
    "model.train()\n",
    "ep = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch_data = batch['input_ids'].to(device)\n",
    "        attention = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Zero the gradients before forward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Use autocast to perform forward pass in TF16 (mixed precision)\n",
    "        with autocast():\n",
    "            outputs = model(batch_data,\n",
    "                            labels=batch_data,\n",
    "                            attention_mask=attention,\n",
    "                            token_type_ids=None\n",
    "                            )\n",
    "\n",
    "            loss = outputs[0]\n",
    "            batch_loss = loss.item()\n",
    "            total_train_loss += batch_loss\n",
    "\n",
    "        # Use scaler to scale the loss and perform backward pass in TF32\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Update parameters using optimizer step and scaler\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(dataloader)\n",
    "    ep += 1\n",
    "    print('Epoch:', ep, 'Average training loss =', avg_train_loss)\n",
    "    total_train_loss = 0  # Reset the total training loss for the next epoch\n",
    "\n",
    "# Rest of the code for text generation...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewPytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
