{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /home/arjun/NewPytorchEnv/bin/python3.10: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('0'), PosixPath('1')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('@/tmp/.ICE-unix/1600,unix/PC'), PosixPath('local/PC')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-ubuntu')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/var/lib/flatpak/exports/share')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer,GPT2LMHeadModel, get_scheduler \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from bitsandbytes.optim import Adam8bit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1 \n",
    "lr = 5e-5\n",
    "batch_size = 1\n",
    "save_loc = '/home/arjun/Documents/ModelSaves/GPT2Alpaca'\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset preparation and tokenising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tatsu-lab--alpaca-2b32f0433506ef5f\n",
      "Found cached dataset parquet (/home/arjun/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75eb7cf9db74d518fbfc81e8aab496d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"tatsu-lab/alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 52002\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset['train']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataset smaller for fast training\n",
    "dataset = dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(save_loc)\n",
    "model =   GPT2LMHeadModel.from_pretrained(save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = {'input_ids':[], 'attention_mask':[]}\n",
    "\n",
    "for example in dataset:\n",
    "    input_text = example['text'].replace('###','')   \n",
    "    encoded_data = tokenizer('<|startoftext|>' + input_text + '<|endoftext|>',truncation=True, max_length=768, padding=\"max_length\")\n",
    "    new_dataset['input_ids'].append(encoded_data['input_ids'])\n",
    "    new_dataset['attention_mask'].append(encoded_data['attention_mask'])\n",
    "\n",
    "new_dataset = Dataset.from_dict(new_dataset)\n",
    "new_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(new_dataset,shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimiser and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = AdamW(model.parameters(), lr=lr)\n",
    "optimizer = Adam8bit(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "num_training_steps = num_epochs * len(dataloader)\n",
    "lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ea3be70a554a91bf61a8e231d5b16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/99 [00:00<?, ?steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'str2optimizer8bit_blockwise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m total_train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\n\u001b[1;32m     20\u001b[0m loss\u001b[39m.\u001b[39mbackward()          \u001b[39m# computes gradients\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()         \u001b[39m# optimises\u001b[39;00m\n\u001b[1;32m     22\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()      \u001b[39m# updates lr according to schedule. Improves performance\u001b[39;00m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()    \u001b[39m# resets the gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/NewPytorchEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/NewPytorchEnv/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/NewPytorchEnv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/optim/optimizer.py:269\u001b[0m, in \u001b[0;36mOptimizer8bit.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_state(group, p, gindex, pindex)\n\u001b[1;32m    268\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetch_state(p)\n\u001b[0;32m--> 269\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_step(group, p, gindex, pindex)\n\u001b[1;32m    270\u001b[0m         torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n\u001b[1;32m    271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_paged:\n\u001b[1;32m    272\u001b[0m     \u001b[39m# all paged operation are asynchronous, we need\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[39m# to sync to make sure all tensors are in the right state\u001b[39;00m\n",
      "File \u001b[0;32m~/NewPytorchEnv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/optim/optimizer.py:517\u001b[0m, in \u001b[0;36mOptimizer2State.update_step\u001b[0;34m(self, group, p, gindex, pindex)\u001b[0m\n\u001b[1;32m    515\u001b[0m     state[\u001b[39m\"\u001b[39m\u001b[39mmax2\u001b[39m\u001b[39m\"\u001b[39m], state[\u001b[39m\"\u001b[39m\u001b[39mnew_max2\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m state[\u001b[39m\"\u001b[39m\u001b[39mnew_max2\u001b[39m\u001b[39m\"\u001b[39m], state[\u001b[39m\"\u001b[39m\u001b[39mmax2\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    516\u001b[0m \u001b[39melif\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mstate1\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39muint8 \u001b[39mand\u001b[39;00m config[\u001b[39m\"\u001b[39m\u001b[39mblock_wise\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 517\u001b[0m     F\u001b[39m.\u001b[39;49moptimizer_update_8bit_blockwise(\n\u001b[1;32m    518\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_name,\n\u001b[1;32m    519\u001b[0m         grad,\n\u001b[1;32m    520\u001b[0m         p,\n\u001b[1;32m    521\u001b[0m         state[\u001b[39m\"\u001b[39;49m\u001b[39mstate1\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    522\u001b[0m         state[\u001b[39m\"\u001b[39;49m\u001b[39mstate2\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    523\u001b[0m         config[\u001b[39m\"\u001b[39;49m\u001b[39mbetas\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    524\u001b[0m         config[\u001b[39m\"\u001b[39;49m\u001b[39mbetas\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m1\u001b[39;49m],\n\u001b[1;32m    525\u001b[0m         config[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    526\u001b[0m         step,\n\u001b[1;32m    527\u001b[0m         config[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    528\u001b[0m         state[\u001b[39m\"\u001b[39;49m\u001b[39mqmap1\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    529\u001b[0m         state[\u001b[39m\"\u001b[39;49m\u001b[39mqmap2\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    530\u001b[0m         state[\u001b[39m\"\u001b[39;49m\u001b[39mabsmax1\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    531\u001b[0m         state[\u001b[39m\"\u001b[39;49m\u001b[39mabsmax2\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    532\u001b[0m         config[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    533\u001b[0m         gnorm_scale\u001b[39m=\u001b[39;49mgnorm_scale,\n\u001b[1;32m    534\u001b[0m         skip_zeros\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mskip_zeros\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    535\u001b[0m     )\n",
      "File \u001b[0;32m~/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/functional.py:1213\u001b[0m, in \u001b[0;36moptimizer_update_8bit_blockwise\u001b[0;34m(optimizer_name, g, p, state1, state2, beta1, beta2, eps, step, lr, qmap1, qmap2, absmax1, absmax2, weight_decay, gnorm_scale, skip_zeros)\u001b[0m\n\u001b[1;32m   1211\u001b[0m is_on_gpu([g, p, state1, state2, qmap1, qmap2, absmax1, absmax2])\n\u001b[1;32m   1212\u001b[0m \u001b[39mif\u001b[39;00m g\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat32 \u001b[39mand\u001b[39;00m state1\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39muint8:\n\u001b[0;32m-> 1213\u001b[0m     optim_func \u001b[39m=\u001b[39m str2optimizer8bit_blockwise[optimizer_name][\u001b[39m0\u001b[39m]\n\u001b[1;32m   1214\u001b[0m \u001b[39melif\u001b[39;00m g\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16 \u001b[39mand\u001b[39;00m state1\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39muint8:\n\u001b[1;32m   1215\u001b[0m     optim_func \u001b[39m=\u001b[39m str2optimizer8bit_blockwise[optimizer_name][\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'str2optimizer8bit_blockwise' is not defined"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps-1),desc='Training', unit='steps')\n",
    "model.train()   # Some layers behave differently to training and inference. This sets all those \n",
    "                 # layers into training mode\n",
    "ep = 0\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss=0\n",
    "    for batch in dataloader:\n",
    "        batch_data = batch['input_ids'].to(device)\n",
    "        attention = batch['attention_mask'].to(device)\n",
    "        model.zero_grad()  \n",
    "        outputs = model(  batch_data,\n",
    "                          labels=batch_data, \n",
    "                          attention_mask = attention,\n",
    "                          token_type_ids=None\n",
    "                        )\n",
    "\n",
    "        loss = outputs[0]     # compute loss \n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "        loss.backward()          # computes gradients\n",
    "        optimizer.step()         # optimises\n",
    "        lr_scheduler.step()      # updates lr according to schedule. Improves performance\n",
    "        optimizer.zero_grad()    # resets the gradients\n",
    "        progress_bar.update(1)   # updates progress bar by 1\n",
    "    avg_train_loss = total_train_loss / len(dataloader)\n",
    "    ep += 1\n",
    "    print('Epoch:',ep,'Average training loss = ',avg_train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Write a poem on cow'\n",
    "\n",
    "prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    " Instruction:{question}\n",
    " Response: \"\"\"\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                generated, \n",
    "                                do_sample=True,   \n",
    "                                top_k=100,\n",
    "                                max_length=100,\n",
    "                                max_new_tokens=200,\n",
    "                                top_p=.95, \n",
    "                                num_return_sequences= 5,\n",
    "                                temperature = .9,\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    ans = tokenizer.decode(sample_output, skip_special_tokens=True).split('Response: ')\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    try:        print(f'<-{i+1}-> {ans[1]}')\n",
    "    except:\n",
    "        print(f'<-{i+1}-> ___No response___')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewPytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
