{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Credits to hugging face documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR) # This blocks all warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline() is the easiest and fastest way to use a pretrained model for inference.\n",
    "Start by creating an instance of pipeline()\n",
    "The pipeline() downloads and caches a default pretrained model and tokenizer for sentiment analysis.\n",
    "The pipeline() can accommodate any model from the Hub, making it easy to adapt the pipeline() for other use-cases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\",\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "classifier([\"I don't think anybody hates cows.\",\"I don't think anybody hate cows.\"]) # huh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline('text-generation', \"gpt2\")\n",
    "print(type(model))\n",
    "print(model(\"Last night, I saw a cow\")[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutomodelForCausalLM and AutoTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While pipeline() is an awesome way to use pre-trained models, it encapsulates all the working, like tokenising of the input, and back. To get all fine-grained control of the whole process, we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Change the model_name to GPT-2\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Example text\n",
    "text = \"I saw a cow in the office, which\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text using GPT-2\n",
    "generated_ids = model.generate(input_ids=inputs[\"input_ids\"], max_length=50, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Checking if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# initialising model and tokenizer for gpt2\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "input_data = 'There was a time when I saw a cow in the office'\n",
    "\n",
    "# Encoding the input and moving tensors to GPU\n",
    "encoding = gpt_tokenizer(input_data, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text using GPT-2\n",
    "generated_ids = gpt_model.generate(input_ids=encoding['input_ids'], max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Move generated IDs to CPU before decoding\n",
    "generated_ids = generated_ids[0].cpu()\n",
    "\n",
    "# Decoding on CPU\n",
    "generated_text = gpt_tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_loc =\"/home/arjun/Desktop/./pt_save\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model.save_pretrained(my_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = AutoModelForCausalLM.from_pretrained(my_loc)\n",
    "# print(loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customising models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "# Create a GPT-2 configuration with attention heads of 10. instead of the default 12\n",
    "gpt2_config = AutoConfig.from_pretrained(\"gpt2\", n_heads=10)\n",
    "\n",
    "# Create a new GPT-2 model with the modified configuration\n",
    "gpt2_model = AutoModel.from_config(gpt2_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models are a standard torch.nn.Module so you can use them in any typical training loop. While you can write your own training loop, ðŸ¤— Transformers provides a Trainer class for PyTorch, which contains the basic training loop and adds additional functionality for features like distributed training, mixed precision, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_function\u001b[39m(examples):\n\u001b[1;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(examples[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m], return_special_tokens_mask\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 25\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(tokenize_function, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[39m# Data collator for language modeling (GPT-2)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m data_collator \u001b[39m=\u001b[39m DataCollatorForLanguageModeling(\n\u001b[1;32m     29\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer, mlm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     30\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, AutoTokenizer, DataCollatorForLanguageModeling, Trainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load GPT-2 model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Training arguments for fine-tuning GPT-2\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"path/to/save/folder/\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    ")\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# Add a new padding token to the tokenizer\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "# Load and tokenize the dataset\n",
    "dataset = ''\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator for language modeling (GPT-2)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "# Create Trainer for fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Fine-tune GPT-2\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# Step 3: Tokenize the dataset using the GPT-2 tokenizer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m tokenized_custom_dataset \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m     15\u001b[0m     custom_dataset,\n\u001b[1;32m     16\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m     truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     18\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[39m# Step 4: Create DataLoader and define training arguments\u001b[39;00m\n\u001b[1;32m     23\u001b[0m train_dataset \u001b[39m=\u001b[39m TextDataset(tokenized_custom_dataset, tokenizer\u001b[39m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/NewPytorchEnv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2829\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2812\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[39mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2825\u001b[0m \u001b[39m        details in `encode_plus`).\u001b[39;00m\n\u001b[1;32m   2826\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2828\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 2829\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_padding_truncation_strategies(\n\u001b[1;32m   2830\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2831\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2832\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2833\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2834\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2835\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2836\u001b[0m )\n\u001b[1;32m   2838\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m   2839\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2840\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2855\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2856\u001b[0m )\n",
      "File \u001b[0;32m~/NewPytorchEnv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2466\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2464\u001b[0m \u001b[39m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   2465\u001b[0m \u001b[39mif\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token_id \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m-> 2466\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2467\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2468\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2469\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpad_token\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m})`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2470\u001b[0m     )\n\u001b[1;32m   2472\u001b[0m \u001b[39m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   2473\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2474\u001b[0m     truncation_strategy \u001b[39m!=\u001b[39m TruncationStrategy\u001b[39m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   2475\u001b[0m     \u001b[39mand\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     \u001b[39mand\u001b[39;00m (max_length \u001b[39m%\u001b[39m pad_to_multiple_of \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m   2479\u001b[0m ):\n",
      "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Step 1: Create a custom dataset in a text file named custom_dataset.txt\n",
    "\n",
    "# Step 2: Load the custom dataset\n",
    "dataset_path = \"custom_dataset.txt\"\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    custom_dataset = f.readlines()\n",
    "\n",
    "# Step 3: Tokenize the dataset using the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenized_custom_dataset = tokenizer.batch_encode_plus(\n",
    "    custom_dataset,\n",
    "    max_length=1024,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# Step 4: Create DataLoader and define training arguments\n",
    "train_dataset = TextDataset(tokenized_custom_dataset, tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-custom-model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Step 5: Initialize the GPT-2 model and the Trainer, and start training\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewPytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
