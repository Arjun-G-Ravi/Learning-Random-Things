{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 20\n",
    "lr = 5e-5\n",
    "batch_size = 14\n",
    "warmup_steps= 750\n",
    "save_loc1 = '/home/arjun/Documents/ModelSaves/GPT2Alpaca'\n",
    "save_loc2 = '/home/arjun/Documents/ModelSaves/GPT2AlpacaNew'\n",
    "save_loc3 = '/home/arjun/Documents/ModelSaves/GPT2Alpaca-midEpoch'\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "dataset = dataset['train']\n",
    "\n",
    "# Making dataset smaller for fast training\n",
    "dataset = dataset.select(range(52000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(save_loc1)\n",
    "model = GPT2LMHeadModel.from_pretrained(save_loc1)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "for example in dataset:\n",
    "    input_text = example['text'].replace('###', '')\n",
    "    encoded_data = tokenizer('' + input_text + '', truncation=True, max_length=768, padding=\"max_length\")\n",
    "    new_dataset['input_ids'].append(encoded_data['input_ids'])\n",
    "    new_dataset['attention_mask'].append(encoded_data['attention_mask'])\n",
    "\n",
    "new_dataset = Dataset.from_dict(new_dataset)\n",
    "new_dataset.set_format(\"torch\")\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(new_dataset, shuffle=True, batch_size=batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "num_training_steps = num_epochs * len(dataloader)\n",
    "lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf64f497c984ab69fc86fee9669337c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/74299 [00:00<?, ?steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-29 23:31:45,576] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-07-29 23:50:03,378] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Average training loss = 1.0319024256990863\n",
      "Epoch: 2 Average training loss = 0.2395178459414242\n",
      "Epoch: 3 Average training loss = 0.22346324833012204\n",
      "Epoch: 4 Average training loss = 0.21141124311157705\n",
      "Epoch: 5 Average training loss = 0.20068976198038535\n",
      "Epoch: 6 Average training loss = 0.1915696698655832\n",
      "Epoch: 7 Average training loss = 0.18382316657102604\n",
      "Epoch: 8 Average training loss = 0.1772697814681328\n",
      "Epoch: 9 Average training loss = 0.17136197922247415\n",
      "Epoch: 10 Average training loss = 0.1662357897829881\n",
      "Epoch: 11 Average training loss = 0.1615352763662749\n",
      "Epoch: 12 Average training loss = 0.1574526953725282\n",
      "Epoch: 13 Average training loss = 0.15371499086252613\n",
      "Epoch: 14 Average training loss = 0.15039131903712555\n",
      "Epoch: 15 Average training loss = 0.14750854182652479\n",
      "Epoch: 16 Average training loss = 0.14495826907867057\n",
      "Epoch: 17 Average training loss = 0.14282140334915183\n",
      "Epoch: 18 Average training loss = 0.1410154115243202\n",
      "Epoch: 19 Average training loss = 0.13957812427110658\n",
      "Epoch: 20 Average training loss = 0.13854403136997492\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "progress_bar = tqdm(range(num_training_steps-1), desc='Training', unit='steps')\n",
    "model.train()\n",
    "ep = 0\n",
    "prev_avg_train_loss = 200\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch_data = batch['input_ids'].to(device)\n",
    "        attention = batch['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(batch_data,\n",
    "                            labels=batch_data,\n",
    "                            attention_mask=attention,\n",
    "                            token_type_ids=None\n",
    "                            )\n",
    "\n",
    "            loss = outputs[0]\n",
    "            batch_loss = loss.item()\n",
    "            total_train_loss += batch_loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(dataloader)\n",
    "    ep += 1\n",
    "    print('Epoch:', ep, 'Average training loss =', avg_train_loss)\n",
    "    if abs(prev_avg_train_loss - avg_train_loss) < 0.0001:\n",
    "        model.save_pretrained(save_loc2)\n",
    "        tokenizer.save_pretrained(save_loc2)\n",
    "        print(\"Loss is very small\")\n",
    "        break\n",
    "    prev_avg_train_loss = avg_train_loss\n",
    "    model.save_pretrained(save_loc3)\n",
    "    tokenizer.save_pretrained(save_loc3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/arjun/Documents/ModelSaves/GPT2AlpacaNew/tokenizer_config.json',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2AlpacaNew/special_tokens_map.json',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2AlpacaNew/vocab.json',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2AlpacaNew/merges.txt',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2AlpacaNew/added_tokens.json',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2AlpacaNew/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(save_loc2)\n",
    "tokenizer.save_pretrained(save_loc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-1-> \n",
      "1. Exercise regularly: Eating a healthy diet and getting plenty of rest are essential for proper health. \n",
      "2. Exercise smarter: Exercise has several benefits, such as reducing inflammation, improving energy levels, and improving digestion. \n",
      "3. Eat more fruits and vegetables: A healthy diet is one of the most nutrient-dense and essential foods we can eat. \n",
      "4. Exercise at night: Taking the time to enjoy the night is essential for developing good habits and energizing muscles. \n",
      "5. Stay informed: Not eating too much food and getting plenty of sleep are important for physical and mental health. \n",
      "6. Exercise at home: Getting enough of both physical and mental exercise helps to maintain physical and mental health.\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-2-> \n",
      "1. Eat a balanced diet of nutrient-rich foods and vegetables.\n",
      "2. Exercise regularly. \n",
      "3. Drink plenty of water. \n",
      "4. Take part in sports or hobbies that are related to your health. \n",
      "5. Take care of your physical health by drinking enough water. \n",
      "6. Take up low-calorie medications. \n",
      "7. Monitor your blood sugar levels. \n",
      "8. Get enough sleep. \n",
      "9. Avoid stinking, excessive caffeine. \n",
      "10. Make sure to wash your hands often. Tigers did not respond to requests for comment on the latest reports from the SAC. Warren, the Saints and Tigers team was not pleased. #S\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-3-> \n",
      "1. Eat healthy meals at least once a week.\n",
      "2. Drink plenty of water throughout the day.\n",
      "3. Exercise regularly.\n",
      "4. Get enough sleep.\n",
      "5. Get enough rest and nutrition.\n",
      "6. Consider getting used to your surroundings.\n",
      "7. Try to stay outside in the sun.\n",
      "8. Minimize your exposure to sunlight.\n",
      "9. Avoid eating fast food.\n",
      "10. Avoid sugary snacks. centrepoint was reported the Hurricanes to be out of action for the season.\n",
      "Oscar said the\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-4-> \n",
      "1. Eat a balanced and nutritious diet: A balanced diet that includes a variety of nutrient-dense fruits and vegetables, lean proteins, whole grains, and healthy fats is an important part of any healthy lifestyle.\n",
      "2. Exercise regularly: Exercise can help boost your energy, as well as boost your mood. This can be achieved by getting regular physical activity like running, biking, or swimming, or by taking an occasional pill or supplement.\n",
      "3. Get regular sleep: Sleep is essential for maintaining your health. Make sure you get at least 7-8 hours of sleep each night to stay alert and focused.\n",
      "4. Exercise at home: Aim to get at least 20 minutes of exercise every day, and get plenty of rest. \n",
      "5. Eat healthily: Eating a balanced diet and getting regular exercise is the best way to stay healthy.\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-5-> \n",
      "1. Start small. Make sure to plan out your day and include snacks and healthy eating habits. \n",
      "2. Start with a balanced diet. Aim for 8-12 meals per day. \n",
      "3. Practice mindfulness. Focus on physical and mental health. \n",
      "4. Stay hydrated. Stay hydrated throughout the day and incorporate snacks throughout the day. \n",
      "5. Make it a workout. Start with light cardio and get plenty of rest.\n",
      "6. Eat healthy foods. This includes eating healthy and getting all the nutrients. \n",
      "7. Practice relaxation techniques. Practice relaxation techniques like yoga, meditation, and guided imagery. \n",
      "8. Practice healthy eating habits. Make sure to take the time to eat fruits, vegetables, and healthy grains. captain Kamara ahead of the\n"
     ]
    }
   ],
   "source": [
    "question = 'Give me tips to be healthy'\n",
    "\n",
    "prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    " Instruction:{question}\n",
    " Response: \"\"\"\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                generated, \n",
    "                                do_sample=True,   \n",
    "                                top_k=100,\n",
    "                                max_length=100,\n",
    "                                max_new_tokens=200,\n",
    "                                top_p=.95, \n",
    "                                num_return_sequences= 5,\n",
    "                                temperature = .9,\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    ans = tokenizer.decode(sample_output, skip_special_tokens=True).split('Response: ')\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    try:        print(f'<-{i+1}-> {ans[1]}')\n",
    "    except:\n",
    "        print(f'<-{i+1}-> ___No response___')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewPytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
