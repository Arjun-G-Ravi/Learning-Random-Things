{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 20\n",
    "lr = 5e-5\n",
    "batch_size = 14\n",
    "warmup_steps= 750\n",
    "save_loc1 = '/home/arjun/Documents/ModelSaves/GPT2Alpaca'\n",
    "save_loc2 = '/home/arjun/Documents/ModelSaves/GPT2AlpacaNew'\n",
    "save_loc3 = '/home/arjun/Documents/ModelSaves/GPT2Alpaca-midEpoch'\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "dataset = dataset['train']\n",
    "\n",
    "# Making dataset smaller for fast training\n",
    "dataset = dataset.select(range(52000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(save_loc1)\n",
    "model = GPT2LMHeadModel.from_pretrained(save_loc1)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "for example in dataset:\n",
    "    input_text = example['text'].replace('###', '')\n",
    "    encoded_data = tokenizer('' + input_text + '', truncation=True, max_length=768, padding=\"max_length\")\n",
    "    new_dataset['input_ids'].append(encoded_data['input_ids'])\n",
    "    new_dataset['attention_mask'].append(encoded_data['attention_mask'])\n",
    "\n",
    "new_dataset = Dataset.from_dict(new_dataset)\n",
    "new_dataset.set_format(\"torch\")\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(new_dataset, shuffle=True, batch_size=batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "num_training_steps = num_epochs * len(dataloader)\n",
    "lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "progress_bar = tqdm(range(num_training_steps-1), desc='Training', unit='steps')\n",
    "model.train()\n",
    "ep = 0\n",
    "prev_avg_train_loss = 200\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch_data = batch['input_ids'].to(device)\n",
    "        attention = batch['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(batch_data,\n",
    "                            labels=batch_data,\n",
    "                            attention_mask=attention,\n",
    "                            token_type_ids=None\n",
    "                            )\n",
    "\n",
    "            loss = outputs[0]\n",
    "            batch_loss = loss.item()\n",
    "            total_train_loss += batch_loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(dataloader)\n",
    "    ep += 1\n",
    "    print('Epoch:', ep, 'Average training loss =', avg_train_loss)\n",
    "    if abs(prev_avg_train_loss - avg_train_loss) < 0.0001:\n",
    "        model.save_pretrained(save_loc2)\n",
    "        tokenizer.save_pretrained(save_loc2)\n",
    "        print(\"Loss is very small\")\n",
    "        break\n",
    "    prev_avg_train_loss = avg_train_loss\n",
    "    model.save_pretrained(save_loc3)\n",
    "    tokenizer.save_pretrained(save_loc3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_loc2)\n",
    "tokenizer.save_pretrained(save_loc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /home/arjun/NewPytorchEnv/bin/python3.10: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('0'), PosixPath('1')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('local/PC'), PosixPath('@/tmp/.ICE-unix/1597,unix/PC')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-ubuntu')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/var/lib/flatpak/exports/share')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "save_loc2 = '/home/arjun/Documents/ModelSaves/GPT2AlpacaNew'\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_loc2)\n",
    "model = GPT2LMHeadModel.from_pretrained(save_loc2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-1->  Albert Einstien is known for his innovative ideas, creative writing, and humanitarian efforts.\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-2->  Albert Einstien was a German painter, sculptor, architect, and scientist who is best known for his pioneering work in the field of paleontology. He is widely recognized as one of the most influential figures in the field of paleontology, and his works are widely considered to be the foundation of modern paleontology.\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-3->  Albert Einstien was a German chemist who developed the first mass spectrometer and the first effective method for measuring the boiling point of water.\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-4->  Albert Einstien was a German-born theoretical physicist and theoretical biologist who developed the theory of relativity.\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-5->  Albert Einstien is known for his innovative and innovative ideas, such as the theory of relativity, the theory of relativity, and the theory of evolution.\n"
     ]
    }
   ],
   "source": [
    "question = 'For what is Albert Einstien known for'\n",
    "\n",
    "prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    " Instruction:{question}\n",
    " Response: \"\"\"\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                generated, \n",
    "                                do_sample=True,   \n",
    "                                top_k=20,\n",
    "                                max_length=200,\n",
    "                                top_p=.9, \n",
    "                                num_return_sequences= 5,\n",
    "                                temperature = .7,\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    ans = tokenizer.decode(sample_output, skip_special_tokens=True).split('Response: ')\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    try:        print(f'<-{i+1}-> {ans[1]}')\n",
    "    except:\n",
    "        print(f'<-{i+1}-> ___No response___')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewPytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
