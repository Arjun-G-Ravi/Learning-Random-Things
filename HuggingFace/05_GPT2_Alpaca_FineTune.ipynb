{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = '/home/arjun/Documents/ModelSaves/GPT2Alpaca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer,GPT2LMHeadModel, get_scheduler \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset preparation and tokenising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tatsu-lab/alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataset smaller for fast training\n",
    "dataset = dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading model and tokeniser from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading tokeniser from save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(save_loc)\n",
    "model =   GPT2LMHeadModel.from_pretrained(save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = {'input_ids':[], 'attention_mask':[]}\n",
    "\n",
    "for example in dataset:\n",
    "    input_text = example['text'].replace('\\n', '').replace('###','\\n')   \n",
    "    encoded_data = tokenizer('<|startoftext|>' + input_text + '<|endoftext|>',truncation=True, max_length=768, padding=\"max_length\")\n",
    "    new_dataset['input_ids'].append(encoded_data['input_ids'])\n",
    "    new_dataset['attention_mask'].append(encoded_data['attention_mask'])\n",
    "\n",
    "new_dataset = Dataset.from_dict(new_dataset)\n",
    "new_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(new_dataset,shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimiser and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 14\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = num_epochs * len(dataloader)\n",
    "lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266fd161ed804ac19ef8c4dbb0f86d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1749 [00:00<?, ?steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Average training loss =  0.27909325844049454\n",
      "Epoch: 2 Average training loss =  0.23163198763132095\n",
      "Epoch: 3 Average training loss =  0.21218926632404328\n",
      "Epoch: 4 Average training loss =  0.1976565789580345\n",
      "Epoch: 5 Average training loss =  0.18594333910942076\n",
      "Epoch: 6 Average training loss =  0.17401826828718187\n",
      "Epoch: 7 Average training loss =  0.16392649263143538\n",
      "Epoch: 8 Average training loss =  0.15621806234121322\n",
      "Epoch: 9 Average training loss =  0.14885319823026658\n",
      "Epoch: 10 Average training loss =  0.14225157314538955\n",
      "Epoch: 11 Average training loss =  0.13674755388498305\n",
      "Epoch: 12 Average training loss =  0.13316831189393996\n",
      "Epoch: 13 Average training loss =  0.1299300209879875\n",
      "Epoch: 14 Average training loss =  0.1280406168103218\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps-1),desc='Training', unit='steps')\n",
    "model.train()   # Some layers behave differently to training and inference. This sets all those \n",
    "                 # layers into training mode\n",
    "ep = 0\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss=0\n",
    "    for batch in dataloader:\n",
    "        # print(batch)\n",
    "        batch_data = batch['input_ids'].to(device)\n",
    "        attention = batch['attention_mask'].to(device)\n",
    "        # batch_data = batch_data.flatten()  # Flatten the tensor to 1-dimensional\n",
    "        # outputs = model.generate(batch_data.unsqueeze(0))\n",
    "        model.zero_grad()  \n",
    "        outputs = model(  batch_data,\n",
    "                          labels=batch_data, \n",
    "                          attention_mask = attention,\n",
    "                          token_type_ids=None\n",
    "                        )\n",
    "\n",
    "        # metric.add_batch(predictions=predictions, references=batch[\"input_ids\"])\n",
    "        loss = outputs[0]     # compute loss \n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "        loss.backward()          # computes gradients\n",
    "        optimizer.step()         # optimises\n",
    "        lr_scheduler.step()      # updates lr according to schedule. Improves performance\n",
    "        optimizer.zero_grad()    # resets the gradients\n",
    "        progress_bar.update(1)   # updates progress bar by 1\n",
    "    avg_train_loss = total_train_loss / len(dataloader)\n",
    "    ep += 1\n",
    "    print('Epoch:',ep,'Average training loss = ',avg_train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving model and tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/arjun/Documents/ModelSaves/GPT2Alpaca.pt/tokenizer_config.json',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2Alpaca.pt/special_tokens_map.json',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2Alpaca.pt/vocab.json',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2Alpaca.pt/merges.txt',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2Alpaca.pt/added_tokens.json',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2Alpaca.pt/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(save_loc)\n",
    "tokenizer.save_pretrained(save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-1->            ‘Duck is king                                                                                                                                                                                        \n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-2-> _______Do you remember when it was all sweet and sweet? When the cows came home, they were all so happy. _______But when they came home, they were all so sad. So why did they come back? They were not giving back what they had given them. And that's when they began to cry. They began to speak, telling stories, sharing food and companionship. Then they were like this: they were going to go back to their old ways and do what they loved. And that's when they were free to do whatever they wanted. So how did they survive that sad fate?\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-3-> ________ _______                                                                                                                                                                                                     \n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-4->  Little milk sweet to sweet to sweet.Tears are shed from my sweetheart's breasts.My heart is full of joy.\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-5-> ’Dear John, I’re writing a poem on your behalf of your beloved. I hope that it can help you understand my love for you and understand my resistance to rejection. It's not only because you're so kind and generous, but also because you're a part of my life and I'm willing to share this journey with you. Your generosity and understanding will enable me to explore the possibilities of your life and embrace the beauty of your innocence. I hope that you will find what I want and that you'll forgive me for being late for something I didn't deserve.\n"
     ]
    }
   ],
   "source": [
    "question = 'Write a poem on cow'\n",
    "\n",
    "prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    " Instruction:{question}\n",
    " Response: \"\"\"\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                generated, \n",
    "                                do_sample=True,   \n",
    "                                top_k=100,\n",
    "                                max_length=100,\n",
    "                                max_new_tokens=200,\n",
    "                                top_p=.95, \n",
    "                                num_return_sequences= 5,\n",
    "                                temperature = .9,\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    ans = tokenizer.decode(sample_output, skip_special_tokens=True).split('Response: ')\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    try:        print(f'<-{i+1}-> {ans[1]}')\n",
    "    except:\n",
    "        print(f'<-{i+1}-> ___No response___')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewPytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
